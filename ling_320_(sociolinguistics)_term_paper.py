# -*- coding: utf-8 -*-
"""LING 320 (Sociolinguistics) Term Paper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nA_SQkcNu1mC1m9TdWp76A6TVemInyxU

# Analyizing Lexical Variation Across Different Reddit Communities

## Step 1: Setup
Loading the relavent libraries we will use later to analyze the data.
"""

!pip install convokit
!pip install sklearn
# spacy setup
!python -m spacy download en_core_web_sm
# nltk setup
import nltk
nltk.download('punkt')

import convokit
from convokit import Corpus, download

import pandas as pd
import numpy as np
from numpy.linalg import norm, multi_dot, inv

import re

"""### Loading our data
We are using Convokit's corpus's of reddit communities. We include data from a more [general corpus](https://convokit.cornell.edu/documentation/reddit-small.html) of 100 highly active subreddits containing 2,000,000 posts. We add too this a few more subreddit's that serve as approximations of reigonal groups.
"""

corpus = Corpus(filename=download('reddit-corpus'))
corpus.print_summary_stats()

"""# Step 2: Binary Lexical Variation
### Binary Relative Frequencies

For our first task, we look at the frequencies of two different words. The pairs of words analyzed should be words that appear in a complimentary distribution ("going to" v.s. "gonna"). By observing the ratio of their frequencies, we can get an idea of how often each word is used.

### Picking Pairs of Words
After some thought, we came up with 3 types of lexical variance to analyze.

* Contractions vs. Non-Contracted Counterparts (10 Pairs)
  * Apostrophes are optional when parsing text

* British vs. American Spellings (10 Pairs)
  * 10 frequent Amrican-British spelling differences
* Internet Colloqualisisms (Acronyms) vs. Unshortened Counterpart  (5 Pairs)
  * We picked 5 that are actually shortening something people say, avoiding acronyms that have their own meaning. Things like LMAO (Laughing my ass off) and LOL (Laughing out Loud) are not used in place of those expressions, and are likely in a distribution of their own.


With our set of words picked, we are ready to analyze. But as we learned later on, it's not as easy as it looks to find pairs of words in complimentary distribution. Part of our analysis is also understanding which of these pairs are not good pairs at all.
"""

contractions = [["gonna", "going to"],
                ["i ain'?t", "i'?m not"],
                ["you'?re", "you are"],
                ["don'?t", "do not"],
                ["isn'?t", "is not"],
                ["he'?s", "he is"],
                ["they'?re", "they are"],
                ["can't", "cannot"],
                ["didn'?t", "did not"],
                ["shouldn't", "should not"]]
contractions_set = set([word1 for word1, word2 in contractions])

british =      [['colour', 'color'],
                ['centre', 'center'],
                ['grey', 'gray'],
                ['programme', 'program'],
                ['theatre', 'theater'],
                ['jewellery', 'jewelry'],
                ['labour', 'labor'],
                ['cheque', 'check']]
british_set = set([word1 for word1, word2 in british])

acronyms =     [["brb", "be right back"],
                ["idk", "i (don'?t|do not) know"],
                ["omg", "oh my god"],
                ["btw", "by the way"],
                ["irl", "in real life"]]
acronyms_set = set([word1 for word1, word2 in acronyms])
binary_words = contractions + british + acronyms

def binary_lexical_variation(word1, word2, corp, meta='subreddit'):
  word_1 = re.compile(word1)
  word_2 = re.compile(word2)
  word_ratio_dict ={}
  # Sub : [# word1, # word2]
  total = 0
  for utt in corp.iter_utterances():
    sub = utt.meta[meta]
    if type(utt.meta[meta]) is str:
      sub = utt.meta[meta].lower()
    else:
      if utt.meta['score'] < 0:
        sub = -1
      else:
        sub = ((utt.meta['score']) // 10) * 10

    total += 1
    if sub not in word_ratio_dict:
      word_ratio_dict[sub] = [0,0]
    text = utt.text.lower()
    for word in re.findall(word_1, text):
      word_ratio_dict[sub][0]+=1
    for word in re.findall(word_2, text):
      word_ratio_dict[sub][1]+=1

  # Calculate Similarities
  for sub, values in word_ratio_dict.items():
    if values[1] != 0:
      word_ratio_dict[sub] = values[0]/values[1] # Adding 1 for Div by 0
    else:
      # If word_2 freq is 0, then the w1 ratio should be the maximum
      word_ratio_dict[sub] = -1
  if max(word_ratio_dict.values()) == 0:
    return 0
  # Normalize Similarities against the Max (0-1 Scale)
  """
  for sub, v in word_ratio_dict.items():
    if v != -1:
      word_ratio_dict[sub] = v / max(word_ratio_dict.values())
    else:
      word_ratio_dict[sub] = 1.0
  """
  return word_ratio_dict # word radio dict -> {subreddits: word_1 / word_2}

"""## Reddit Comparison
This code goes through each pair in our set, and finds the number of times that word is used in each subreddit. It stores frequency values given by the binary_lexical_variation fn into a Pandas Data Frame.
"""

flag = True
for word_1, word_2 in binary_words:
  output = binary_lexical_variation(word_1, word_2, corpus)
  if flag:
    sub_dict = {k: [v]  for k,v in output.items()}
    flag = False
  else:
    print(word_1, word_2)
    print(sub_dict)
    if output == 0:
       binary_words.remove([word_1,word_2])
       continue
    sub_dict = {k: sub_dict[k] + [v] for k,v in output.items()}
binary_df = pd.DataFrame.from_dict(sub_dict)
binary_df.index= [word1 + "/" + word2 for word1,word2 in binary_words]

binary_df

"""We then store the Data Frame into a CSV for our analysis."""

from google.colab import drive
drive.mount('/content/drive/')

binary_df.to_csv("/content/drive/My Drive/binary_variation.csv")

"""## Lexical Variation as it relates to score.

Here, we change the optional "meta" paramter to score. Instead of analyizing lexical varaition with respsect to subreddits, we analyze with respect to what the post scored with users. We store the relative frequency of each pair, at each score.
"""

flag = True
none_found = set()
for word_1, word_2 in binary_words:
  output = binary_lexical_variation(word_1, word_2, corpus, meta="score")
  if flag:
    sub_dict = {k: [v]  for k,v in output.items()}
    flag = False
  else:
    if output == 0:
      print([word_1,word_2] in binary_words)
      binary_words.remove([word_1,word_2])
      continue
    sub_dict = {k: sub_dict[k] + [v] for k,v in output.items()}

binary_df = pd.DataFrame.from_dict(sub_dict)
binary_df.index= [word1 + "/" + word2 for word1,word2 in binary_words]

binary_df

from google.colab import drive
drive.mount('/content/drive/')

binary_df.to_csv("/content/drive/My Drive/mcgillcontractions.csv")

"""## Step 3: Determing Subreddit Similarity
From Source: "The data is extracted from publicly available Reddit data of 2.5 years from Jan 2014 to April 2017.

This file generates one numerical vector in low dimensional space (a.k.a. embeddings) for each subreddit. The embeddings are 300 dimensions each. Two subreddit embeddings are similar if the users who post in them are similar."

**TLDR**: *Embeddings were created by looking at user overlap, and by taking the cosine similarity of two reddit's embeddings, we get a score of their similarity*

Academic Paper: (https://cs.stanford.edu/~srijan/pubs/conflict-paper-www18.pdf)

Data Source: (https://snap.stanford.edu/data/web-RedditEmbeddings.html)
"""

import numpy as np
from numpy.linalg import norm, multi_dot, inv
def get_overlaps(U, Sigma, VT, subreddit, k, df):
    #    U     SIGMA     VT
    # (len(df.index), 30), (30,), (30, 300))
    doc_index = df.index.get_loc(subreddit)
    Sigma = np.diag(Sigma)
    compareDocLow = Sigma.dot(U[doc_index])
    Norm1 = norm(compareDocLow)

    docDict = {}
    print(subreddit)
    for i, doc in enumerate(U):
        if i == doc_index:
            continue
        docLow = Sigma.dot(doc)
        Norm2 = norm(docLow)
        docDict[i] = np.dot(compareDocLow,docLow) / (Norm1 * Norm2)
    docDict = {df.index[k]: v for k,v in sorted(docDict.items(), key=lambda item: item[1], reverse=True)[:k]}


    return docDict

from sklearn.utils.extmath import randomized_svd
norm_df = binary_df
subreddits = list(norm_df.columns)
reddit_emeddings = reddit_emeddings[reddit_emeddings.index.isin(subreddits)]
U, Sigma, VT = randomized_svd(reddit_emeddings, n_components=15, n_iter=100, random_state=42)
U.shape, Sigma.shape, VT.shape

overlaps = {}
for subreddit in reddit_emeddings.index:
    overlaps1 = get_overlaps(U, Sigma, VT, subreddit, 5, reddit_emeddings)
    overlaps[subreddit] = overlaps1
overlaps

def linguistic_sim(subreddit, l,gg):

  subDict = {}
  Norm1 = norm(list(gg[subreddit]))

  for i, sub in enumerate(gg.columns):
    if sub == subreddit:
      continue
    Norm2 = norm(list(gg[sub]))
    norms = (Norm1 * Norm2)
    subDict[sub] = np.dot(Norm1,Norm2) / (Norm1*Norm2)
  subDict = {k: v for k,v in sorted(subDict.items(), key=lambda item: item[1], reverse=True) }
  return subDict

def graphData(df):
  normDict = []
  for i, sub in enumerate(df.columns):
    # norm of the subreddit higher more contracted
    normDict.append([sub, overlaps[sub], np.average([int(100*float(df.loc["gonna/going to",sub])), int(100*float(df.loc["they'?re/they are",sub])) ,int(100*float(df.loc["can't/cannot",sub]))])])
  nd = pd.DataFrame(normDict, columns=["subreddit", "overlaps", "british"])
  return nd

nd = graphData(norm_df)

index = nd.index
nd = nd.sort_values(by='british', ascending=False)
nd.index = index
nd

import ast
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.colors import LogNorm
# Read in the dataframe


# create graph object
G = nx.Graph()
color_map = []
color_map2 = []
# add edges between nodes
for index, row in nd.iterrows():
    subreddit = row['subreddit']
    overlap = row['overlaps']
    #sim = row['linguistic similarity']
    # GRAPHS SIM REDDIT

    for overlap, weight in overlaps[subreddit].items():
      G.add_edge(subreddit, overlap, weight=float(weight))
   #for overlap, weight in sim.items():
     # G2.add_edge(subreddit, overlap, weight=float(weight))
    color_map.append(round(float(row['british']),2))
    #color_map2.append(float(row['gonna']))
    """
    #GRAPH'S CONTRACTION SIM
    for overlap, cont_weight in contraction.items():
        G.add_edge(subreddit, overlap, weight=float(weight))
    """


print(color_map)
# Draw the graph

fig, ax = plt.subplots(figsize=(75, 75))
pos = nx.spring_layout(G,iterations=200)

num_edges = dict(G.degree())
num_labels = dict(G.nodes())

cmap = plt.cm.RdBu


#nx.draw_networkx_nodes(G, pos, node_size=5000,ax=ax, node_shape="o", node_color="black")
nx.draw_networkx_nodes(G, pos, node_size=5000,ax=ax, node_shape="o", cmap=cmap, node_color=range(100))
#nx.draw_networkx_nodes(G2, pos2, node_size=2500,ax=ax, node_shape="o", cmap=cmap, node_color=cmap_values)
#nx.draw_networkx_nodes(G, pos, node_size=3500,ax=ax, node_shape="o", node_color=list(pol_color.values()), cmap=cmap2)
nx.draw_networkx_edges(G, pos, edge_color='grey')
nx.draw_networkx_labels(G, pos, font_size=18, font_family='sans-serif', font_weight="bold",bbox=dict(facecolor='white') )
#nx.draw_networkx_labels(G2, pos2, font_size=18, font_family='sans-serif', font_weight="bold",bbox=dict(facecolor='blue') )
#edge_labels = {(u, v): float(d.values()) for u, v, d in G.edges(data=True)}
#nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)



plt.axis('off')
plt.show()

newheaders = binary_df.iloc[0]
binary_df = binary_df[1:]
binary_df.columns =newheaders
binary_df

"""# Step 4: Graphing Binary Lexical Variation and Similiar Subreddits

Here we are using the python networkx library to plot our findings. Each node is a subreddit, and each edge connects similiar subreddits. Closer nodes are more similiar, and vice versa. Then we color each node based on the average score of each Binary Variation group (Contractions, British/American, Acronyms).

## Reigonal Lexical Variation
An earlier assignment in our class was to survey people we know across North America to analyze reigonal variation. Our survey can be found here: https://forms.gle/SuFa9vxjdAEsfNGf7. We selected the most interesting questions from our survey and analyzed different subreddits. We hope to find correlations between different reigons and subreddits.
"""

entry1 = ["silverware", "utensils", "cutlery"]
entry2 = ["foot[- ]long", "grinder", "hero", "hoagie", "sub"]
#entry3 = ["water fountain", "drinking fountain"]
entry4 = ["coke",  "cold", "drink", "fizzy drink", "[^\w]pop[^\w]", "soda", "soft drink", "tonic"]
entry5 = ["all[- ]dressed", "deluxe", "everything[- ]on[- ]it", "loaded", "supreme", "the works"]
entry6 = ["elementary school", "grade school", "grammar school", "primary school", "public school"]
entry7 = ["bathroom", "restroom"]
entry8 = ["corner shop", "corner store", "deli", "([^\w]dep[^\w]|[^\w]depanneur[^\w])", "variety store"]
entry9 = ["chesterfield", "couch", "davenport", "divan", "settee", "sofa"]

# Double Negatives

reigonal_variation = [entry1, entry2, entry4, entry5, entry6, entry7, entry8, entry9]
all_words = []
for line in reigonal_variation:
  for word in line:
    all_words.append(word)
all_words

"""Our code from earlier modified for non-binary variation."""

flag = True
subreddits = {}

print(all_words)
for words in reigonal_variation:
  regex_list = [re.compile(word) for word in words]
  for utt in corpus.iter_utterances():
    sub = utt.meta['subreddit'].lower()
    if sub in subreddits:
      if words[0] not in subreddits[sub]:
        subreddits[sub].update({word: 0 for word in words})
    else:
      subreddits[sub] = {word: 0 for word in words}
    text = utt.text.lower() # our version of preprocessing
    for i, reg in enumerate(regex_list):
      if re.search(reg, text): # If there was a regex match of the word
        subreddits[sub][words[i]] += 1
  max_freq = {k: 0.0 for k in words}
  min_freq = {k: np.inf for k in words}

  for sub, values in subreddits.items():
      word_sub_values = [subreddits[sub][word] for word in words]
      if sum(word_sub_values) == 0:
        continue
      for word in words:
        value = subreddits[sub][word]
        subreddits[sub][word] = value/sum(word_sub_values) # w freq over total freq
        if value > max_freq[word]:
          max_freq[word] = value
        if value < min_freq[word]:
          min_freq[word] = value
  print(subreddits)

subreddits

complex_dict = pd.DataFrame.from_dict(subreddits)
concat_df = pd.concat([complex_dict, binary_df])
norm_df = concat_df.div(concat_df.sum(axis=1), axis=0)
norm_df

dataf=((concat_df-concat_df.min())/(concat_df.max()-concat_df.min()))
dataf

from google.colab import drive
drive.mount('/content/drive/')

norm_df.to_csv("/content/drive/My Drive/norm_df.csv")
dataf.to_csv("/content/drive/My Drive/dataf.csv")

from google.colab import drive
drive.mount('/content/drive/')
reddit_emeddings = pd.read_csv("/content/web-redditEmbeddings-subreddits.csv",index_col=0, header=None)
binary_df = pd.read_csv("/content/drive/My Drive/binarylexicalvariation2.csv",index_col=0, header=None)
binary_df

"""## Reigonal Lexical Variation Analysis
Computing the relative frequencies of words from the questionaire used in the Data Validation assignment.
"""

corpora =     ['reddit-corpus',           # Corpus with 100 Subreddits and a total of 2,000,000+ Posts
               'subreddit-mcgill',        # McGill Speakers
               'subreddit-AskUK',         # UK Speakers
               'subreddit-Pennsylvania',  # PA Speakers
               'subreddit-AskNYC',        # New York Speakers
               'subreddit-bostonhousing']        # Boston Speakers

for corpus in corpora[1:]:
  temp = Corpus(filename=download(corpus))
  temp.print_summary_stats()
  corpus = Corpus.merge(corpus, temp)

"""Answers_list contains a list of words that appear in a complementary distribution.

This is a collection of survey answer choices from a linguistic study of regional variation in North American English, conducted as part of the course LING 320, Sociolinguistics 1, at McGill University, in Montreal, Canada. Survey created by Prof. Charles Boberg.

An example of the survey can be found here: https://docs.google.com/forms/d/1KFzDdqOx9c8QlxxmDo-ST6AFaiq9KteCDTj6t3vOUtY/edit
"""

answers_list = [['facuet', 'tap', 'spigot', 'faucet'],
                ['water fountain', 'drinking fountain', 'bubbler'],
                ['bucket', 'pail'],
                ['gutters', 'eavestroughs'],
                ['soda', 'pop', 'soft drink', 'fizzy drink', 'coke', 'cold drink'],
                ['everything-on-it',
                'supreme',
                'the works',
                'all-dressed',
                'deluxe',],
                ['foot-long', 'sub', 'hoagie', 'hero', 'grinder'],
                ['frosting', 'icing'],
                ['dinner', 'supper', 'tea'],
                ['utensils', 'silverware', 'cutlery'],
                ['washcloth', 'face cloth', 'face flannel'],
                ['sneakers',
                'tennis shoes',
                'running shoes',
                'trainers',
                'runners',
                'gym shoes'],
                ['dressing gown', 'robe', 'bathrobe', 'housecoat'],
                ['romper', 'jumpsuit', 'playsuit', 'onesie'],
                ['dresser', 'bureau', 'chest of drawers'],
                ['couch', 'sofa', 'divan', 'settee'],
                ['see[- ]saw', 'teeter[- ]totter'],
                ['elementary school', 'grade school', 'primary school', 'grammar school'],
                ['first grade', 'grade one', 'year one', 'first form'],
                ['grade', 'mark'],
                ['backpack', 'schoolbag', 'rucksack', 'bookbag', 'satchel', 'knapsack'],
                ['notebook', 'excercise book', 'copybook'],
                ['toiletry bag',
                'toiletry kit',
                'toilet bag',
                'wash bag',
                'toilet case',
                'toilet kit',
                'dopp kit'],
                ['purse', 'pocketbook', 'handbag', 'bag'],
                ['wallet', 'billfold'],
                ['cottage', 'cabin', 'lake house', 'summer house', 'camp', 'chalet'],
                ['living room',
                'family room',
                'sitting room',
                'front room',
                'lounge',
                'parlor'],
                ['studio',
                'bed-sit',
                'bachelor',
                'efficiency',
                #'(one|two|three|1|2|3)&&([- ]and[- ]a[- ]half',
                'loft'],
                ['bathroom',
                'restroom',
                'lavaroty',
                "(men'?s|women'?s) room",
                'toilet',
                'washrrom',
                'the loo',
                "gent'?s|ladie'?s",
                'powder room',
                '[^\w]wc[^\w]'],
                ['convenience store',
                'corner store',
                'deli',
                'bodega',
                'corner shop',
                'd[Ã©e]panneur|dep'],
                ['(parking)?garage', 'car park', 'parkade', 'parking deck'],
                ['(the|a) check-out',
                '(the|a) cashier',
                '(the|a) counter',
                '(the|a) register',
                '(the|a) till',
                '(the|a) cash'],
                ['cashpoint', 'hole[- ]in[- ]the[- ]wall', 'cash[- ]machine', 'bank[- ]machine']]

flag = True
subreddits = {}
word_counts = {}
for i, words in enumerate(answers_list):
  print(words)
  regex_list = [re.compile(word) for word in words]
  for utt in corpus.iter_utterances():
    sub = utt.meta['subreddit'].lower()
    if sub in subreddits:
      if words[0] not in subreddits[sub]:
        subreddits[sub].update({word: 0.0 for word in words})
    else:
      subreddits[sub] = {word: 0.0 for word in words}
    text = utt.text.lower() # our version of preprocessing
    for i, reg in enumerate(regex_list):
      if re.search(reg, text): # If there was a regex match of the word
        subreddits[sub][words[i]] += 1
  max_freq = {k: 0.0 for k in words}
  min_freq = {k: np.inf for k in words}

  for sub, values in subreddits.items():
      word_sub_values = [subreddits[sub][word] for word in words]
      if sum(word_sub_values) == 0:
        continue
      # Triple For Loop... OH NOOOOOO
      for word in words:
        value = subreddits[sub][word]
        word_counts[word] = value
        subreddits[sub][word] = value/sum(word_sub_values) # w freq over total freq
        if value > max_freq[word]:
          max_freq[word] = value
        if value < min_freq[word]:
          min_freq[word] = value

#subreddits["word counts"] = word_counts
#all_reigonal = pd.DataFrame.from_dict(subreddits)
#norm_all_reigonal = all_reigonal.div(all_reigonal.sum(axis=1), axis=0)
#word_counts

from google.colab import drive
drive.mount('/content/drive/')

all_reigonal.to_csv("/content/drive/My Drive/all_reigonal.csv")